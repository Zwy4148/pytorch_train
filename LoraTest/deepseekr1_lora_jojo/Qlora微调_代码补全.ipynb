{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 导入所需库"
      ],
      "metadata": {
        "id": "JXhRVugZVgVd"
      },
      "id": "JXhRVugZVgVd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install modelscope"
      ],
      "metadata": {
        "id": "SF29bu2A4_24"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SF29bu2A4_24"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下载模型"
      ],
      "metadata": {
        "id": "UJn96RsGJgMz"
      },
      "id": "UJn96RsGJgMz"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
        "# 从modelscope下载deepseek r1 1.5B\n",
        "def download_model():\n",
        "    model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', cache_dir='./Model/ds1b/autodl-tmp',\n",
        "                                  revision='master')\n",
        "download_model()"
      ],
      "metadata": {
        "id": "6lYHn1asJmKs"
      },
      "id": "6lYHn1asJmKs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 测试未微调模型补全代码功能"
      ],
      "metadata": {
        "id": "sWTBYrH3VOKu"
      },
      "id": "sWTBYrH3VOKu"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "#下载的模型目录\n",
        "model_path = '/root/autodl-tmp/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B'\n",
        "\n",
        "# 加载tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# 加载模型\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
        "\n",
        "#问题信息\n",
        "messages=[\n",
        "    {'role': 'sysrem', 'content': \"Please complete the following code\"},\n",
        "    { 'role': 'user', 'content': '''def get_logger(name: str) -> logging.Logger:\\n\n",
        "                                        logger = logging.getLogger(name)\\n\n",
        "                                        handler = logging.StreamHandler(sys.stdout)\\n'''}\n",
        "]\n",
        "#转换成模型可接受的输入模版\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "#模型生成输出\n",
        "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "#打印输出内容\n",
        "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ybgPm2cSVTwK"
      },
      "id": "ybgPm2cSVTwK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "de53995b-32ed-4722-8cac-ba104c8efacb",
      "metadata": {
        "id": "de53995b-32ed-4722-8cac-ba104c8efacb"
      },
      "source": [
        "#导入环境和数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fac949-4150-4091-b0c3-2968ab5e385c",
      "metadata": {
        "tags": [],
        "id": "52fac949-4150-4091-b0c3-2968ab5e385c"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
        "# 将JSON文件转换为CSV文件\n",
        "df = pd.read_json(\"/content/all_dataset.json\")\n",
        "ds = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d05e5d-d14e-4f03-92be-9a9677d41918",
      "metadata": {
        "id": "51d05e5d-d14e-4f03-92be-9a9677d41918"
      },
      "source": [
        "#处理数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
      "metadata": {
        "tags": [],
        "id": "74ee5a67-2e55-4974-b90e-cbf492de500a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "model_path = '/content/Model/root/autodl-tmp/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'\n",
        "# 加载分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "# 搭建数据集处理函数\n",
        "def process_func(example):\n",
        "    MAX_LENGTH = 512\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "    instruction = tokenizer((f\"<｜begin▁of▁sentence｜>{example['coder_file_name']}\\n\"\n",
        "                             f\"User: {example['prerequisite'] + example['pre_code']}\\nAssistant: \"\n",
        "                             ).strip(),\n",
        "                            add_special_tokens=False)\n",
        "    response = tokenizer(f\"{example['output']}<｜end▁of▁sentence｜>\", add_special_tokens=False)\n",
        "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
        "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
        "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
        "    if len(input_ids) > MAX_LENGTH:\n",
        "        input_ids = input_ids[:MAX_LENGTH]\n",
        "        attention_mask = attention_mask[:MAX_LENGTH]\n",
        "        labels = labels[:MAX_LENGTH]\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "\n",
        "# 利用处理函数处理数据集\n",
        "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424823a8-ed0d-4309-83c8-3f6b1cdf274c",
      "metadata": {
        "id": "424823a8-ed0d-4309-83c8-3f6b1cdf274c"
      },
      "source": [
        "#加载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "170764e5-d899-4ef4-8c53-36f6dec0d198",
      "metadata": {
        "tags": [],
        "id": "170764e5-d899-4ef4-8c53-36f6dec0d198"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
        "# 以下采用对模型进行量化后加载模型\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.half,\n",
        "                      low_cpu_mem_usage=True,  # 是否使用低CPU内存\n",
        "                      load_in_4bit=True,  # 是否在4位精度下加载模型。如果设置为True，则在4位精度下加载模型。\n",
        "                      bnb_4bit_compute_dtype=torch.half, # 4位精度计算的数据类型。这里设置为torch.half，表示使用半精度浮点数。\n",
        "                      bnb_4bit_quant_type=\"nf4\",  # 4位精度量化的类型。这里设置为\"nf4\"，表示使用nf4量化类型。\n",
        "                      bnb_4bit_use_double_quant=True,  # 是否使用双精度量化。如果设置为True，则使用双精度量化。\n",
        "                      device_map=\"cuda:0\").eval()\n",
        "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n",
        "#enable_input_require_grads 方法的作用就是将模型输入张量的 requires_grad 属性设置为 True，使得在反向传播过程中能够计算输入张量的梯度"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d71257-3c1c-4303-8ff8-af161ebc2cf1",
      "metadata": {
        "id": "13d71257-3c1c-4303-8ff8-af161ebc2cf1"
      },
      "source": [
        "#配置lora参数和转换模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
      "metadata": {
        "tags": [],
        "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "# lora配置\n",
        "config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # 需要训练的层数\n",
        "    target_modules=[\"q_proj\", \"kv_a_proj_with_mqa\", \"kv_b_proj\", \"o_proj\", 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    inference_mode=False, # 训练模式\n",
        "    r=8, # Lora 秩\n",
        "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
        "    lora_dropout=0.1# Dropout 比例\n",
        ")\n",
        "# 将一个预训练模型转换为支持参数高效微调的模型\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca055683-837f-4865-9c57-9164ba60c00f",
      "metadata": {
        "id": "ca055683-837f-4865-9c57-9164ba60c00f"
      },
      "source": [
        "# 配置训练参数并开始训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
      "metadata": {
        "tags": [],
        "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0"
      },
      "outputs": [],
      "source": [
        "# 训练配置\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./output/deepseek_r1\", #lora输出目录\n",
        "    per_device_train_batch_size=1, #训练批次\n",
        "    gradient_accumulation_steps=8, #多少次进行一次梯度累积\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=2, #数据集被完整遍历的次数\n",
        "    save_steps=1000, #每1000次训练集保存一次\n",
        "    learning_rate=1e-5,\n",
        "    save_on_each_node=True,\n",
        "    gradient_checkpointing=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96",
      "metadata": {
        "tags": [],
        "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96"
      },
      "outputs": [],
      "source": [
        "#配置训练\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_id,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
        ")\n",
        "#开始训练\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9823e3c7",
      "metadata": {
        "id": "9823e3c7"
      },
      "source": [
        "#加载 lora 权重并进行测试推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12dad881",
      "metadata": {
        "id": "12dad881"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "#下载的模型目录\n",
        "model_path = '/root/autodl-tmp/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B'\n",
        "#训练完的lora目录\n",
        "lora_path = './output/deepseek_r1/checkpoint-10000'\n",
        "\n",
        "# 加载tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# 加载模型\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
        "\n",
        "# 加载lora权重\n",
        "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
        "#问题信息\n",
        "messages=[\n",
        "    {'role': 'sysrem', 'content': \"Please complete the following code\"},\n",
        "    { 'role': 'user', 'content': '''def get_logger(name: str) -> logging.Logger:\\n\n",
        "                                        logger = logging.getLogger(name)\\n\n",
        "                                        handler = logging.StreamHandler(sys.stdout)\\n'''}\n",
        "]\n",
        "#转换成模型可接受的输入模版\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "#模型生成输出\n",
        "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "#打印输出内容\n",
        "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "UJn96RsGJgMz"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}